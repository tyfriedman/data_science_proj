# -*- coding: utf-8 -*-
"""Decision_Tree_Mariam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Saz225FKQjM1AilXc68hA-Tm9DZetweF

#Libraries
"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn import tree
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""#Cohort 1"""

#Loading cohort 1 file
df = pd.read_csv('../cohorts/cohort1.csv')  # Replace with your actual file

# Separate features and target: whether the class is dropped
X = df.drop('dropped_after_test_1', axis=1)
y = df['dropped_after_test_1']

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train decision tree
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for positive class
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Performance Visualizations for Cohort 1
# 1. Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=["Stayed", "Dropped"], 
            yticklabels=["Stayed", "Dropped"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Cohort 1')
plt.savefig("./visualizations/cohort_1_confusion_matrix.png")
plt.close()

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Cohort 1')
plt.legend(loc="lower right")
plt.savefig("./visualizations/cohort_1_roc_curve.png")
plt.close()

# 3. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Cohort 1')
plt.savefig("./visualizations/cohort_1_precision_recall_curve.png")
plt.close()

#Visualize the decision tree
plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=X.columns, class_names=["Stayed", "Dropped"], filled=True)
plt.savefig("./visualizations/cohort_1_decision_tree.png")
plt.close()

# Get feature importances
importances = clf.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Print the table
print(feat_imp_df)

#Plot the importances
plt.figure(figsize=(10, 6))
plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Most important on top
plt.title('Feature Importance in Decision Tree')
plt.xlabel('Importance Score')
plt.savefig("./visualizations/cohort_1_feature_importance.png")
plt.close()

"""Understanding: Early attendance is key in predicting in whether a student will drop a course in terms of both attending office hours, and being a part of session 1.

#Cohort 2
"""

#Loading cohort 2 file
df = pd.read_csv('../cohorts/cohort2.csv')  # Replace with your actual file

# Separate features and target: whether the class is dropped
X = df.drop('dropped_after_test_2', axis=1)
y = df['dropped_after_test_2']

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train decision tree
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for positive class
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Performance Visualizations for Cohort 2
# 1. Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=["Stayed", "Dropped"], 
            yticklabels=["Stayed", "Dropped"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Cohort 2')
plt.savefig("./visualizations/cohort_2_confusion_matrix.png")
plt.close()

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Cohort 2')
plt.legend(loc="lower right")
plt.savefig("./visualizations/cohort_2_roc_curve.png")
plt.close()

# 3. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Cohort 2')
plt.savefig("./visualizations/cohort_2_precision_recall_curve.png")
plt.close()

#Visualize the decision tree
plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=X.columns, class_names=["Stayed", "Dropped"], filled=True)
plt.savefig("./visualizations/cohort_2_decision_tree.png")
plt.close()

# Get feature importances
importances = clf.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Print the table
print(feat_imp_df)

# Optional: Plot the importances
plt.figure(figsize=(10, 6))
plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Most important on top
plt.title('Feature Importance in Decision Tree')
plt.xlabel('Importance Score')
plt.savefig("./visualizations/cohort_2_feature_importance.png")
plt.close()

"""Understanding: By cohort 2 we see a big change in that attendance in classes or office hours isn't the key point of staying in the class. Rather, posting questions in the forum and having it be answered is the highest indicator of a student dropping after a class. This may indicate that students who were previously struggling already and didn't receive the help the 1 on 1 support they needed ended up dropping.

#Cohort 3
"""

#Loading cohort 3 file
df = pd.read_csv('../cohorts/cohort3.csv')  # Replace with your actual file

# Separate features and target: whether the class is dropped
X = df.drop('dropped_after_test_3', axis=1)
y = df['dropped_after_test_3']

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train decision tree
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for positive class
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Performance Visualizations for Cohort 3
# 1. Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=["Stayed", "Dropped"], 
            yticklabels=["Stayed", "Dropped"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Cohort 3')
plt.savefig("./visualizations/cohort_3_confusion_matrix.png")
plt.close()

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Cohort 3')
plt.legend(loc="lower right")
plt.savefig("./visualizations/cohort_3_roc_curve.png")
plt.close()

# 3. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Cohort 3')
plt.savefig("./visualizations/cohort_3_precision_recall_curve.png")
plt.close()

#Visualize the decision tree
plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=X.columns, class_names=["Stayed", "Dropped"], filled=True)
plt.savefig("./visualizations/cohort_3_decision_tree.png")
plt.close()

# Get feature importances
importances = clf.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Print the table
print(feat_imp_df)

# Optional: Plot the importances
plt.figure(figsize=(10, 6))
plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Most important on top
plt.title('Feature Importance in Decision Tree')
plt.xlabel('Importance Score')
plt.savefig("./visualizations/cohort_3_feature_importance.png")
plt.close()

"""Understanding: By cohort 3, we see attendance in sessions is not as important a factor in predicting drop out rates compared to participation in office ours and having questions answered in the forum.

#Cohort 4
"""

#Loading cohort 4 file
df = pd.read_csv('../cohorts/cohort4.csv')  # Replace with your actual file

# Separate features and target: whether the class is dropped
X = df.drop('final_dropout', axis=1)
y = df['final_dropout']

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train decision tree
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for positive class
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Performance Visualizations for Cohort 4
# 1. Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=["Stayed", "Dropped"], 
            yticklabels=["Stayed", "Dropped"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Cohort 4')
plt.savefig("./visualizations/cohort_4_confusion_matrix.png")
plt.close()

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Cohort 4')
plt.legend(loc="lower right")
plt.savefig("./visualizations/cohort_4_roc_curve.png")
plt.close()

# 3. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Cohort 4')
plt.savefig("./visualizations/cohort_4_precision_recall_curve.png")
plt.close()

#Visualize the decision tree
plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=X.columns, class_names=["Stayed", "Dropped"], filled=True)
plt.savefig("./visualizations/cohort_4_decision_tree.png")
plt.close()

# Get feature importances
importances = clf.feature_importances_
feature_names = X.columns

# Create a DataFrame for better visualization
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Print the table
print(feat_imp_df)

# Optional: Plot the importances
plt.figure(figsize=(10, 6))
plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Most important on top
plt.title('Feature Importance in Decision Tree')
plt.xlabel('Importance Score')
plt.savefig("./visualizations/cohort_4_feature_importance.png")
plt.close()

"""Understanding: This shows that office hour visits were a key indicator of drop out rates. Overall, after cohort 1 session 1 attendance, the overall trend was that 1 on 1 indicators like office hours and forum and question answers were key indicators of drop out rates which means in understanding how to lower drop out rates, it would make sense for an emphasis to be placed on office hours.  """
